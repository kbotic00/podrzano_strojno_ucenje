{"cells":[{"cell_type":"markdown","metadata":{"id":"98AkYCb6sCVO"},"source":["# Uvod #\n","\n","U svijetu dubokog učenja postoji više od pukih gustih slojeva. Postoje deseci vrsta slojeva koje možete dodati modelu. (Probajte pregledavati [Keras docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/) za uzorak!) Neki su poput gustih slojeva i definiraju veze između neurona, a drugi mogu raditi pretprocesiranje ili transformacije drugih vrsta.\n","\n","U ovoj lekciji naučit ćemo o dvije vrste posebnih slojeva koji sami ne sadrže neurone, ali dodaju neke funkcije koje ponekad mogu koristiti modelu na razne načine. Oba se obično koriste u modernim arhitekturama.\n","\n","# Dropout #\n","\n","Prvi od njih je \"ispadajući sloj\", koji može pomoći u ispravljanju prekomjernog postavljanja.\n","\n","U prošloj vježbi govorili smo o tome kako je overfitting uzrokovan mrežom koja uči lažne obrasce u podacima o obuci. Da bi prepoznali ovi lažni obrasci, mreža će se često oslanjati na vrlo specifične kombinacije težine, svojevrsnu \"zavjeru\" težina. Budući da su tako specifični, obično su krhki: uklonite jedan i zavjera se raspada.\n","\n","Ovo je ideja iza **dropouta**. Kako bismo razbili ove zavjere, nasumično *ispuštamo* dio ulaznih jedinica sloja u svakom koraku obuke, što mreži otežava učenje tih lažnih obrazaca u podacima obuke. Umjesto toga, mora tražiti široke, općenite obrasce, čiji su obrasci težine obično robusniji.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"figs/a86utxY.gif\" width=\"600\" alt=\"An animation of a network cycling through various random dropout configurations.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Ovdje je 50% ispadanja dodano između dva skrivena sloja.</center></figcaption>\n","</figure>\n","\n","O ispadanju možete razmišljati i kao o stvaranju neke vrste *ansambla* mreža. Predviđanja više neće raditi jedna velika mreža, već odbor manjih mreža. Pojedinci u odboru skloni su činiti različite vrste grešaka, ali su u isto vrijeme u pravu, čineći odbor u cjelini boljim od bilo kojeg pojedinca. (Ako ste upoznati s random forest kao skupom stabala odlučivanja, to je ista ideja.)\n","\n","## Dodavanje dropout-a ##\n","\n","U Kerasu, argument stope ispadanja `rate` definira koji postotak ulaznih jedinica treba isključiti. Stavite sloj `Dropout` neposredno prije sloja na koji želite primijeniti ispadanje:\n","\n","```\n","keras.Sequential([\n","    # ...\n","    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n","    layers.Dense(16),\n","    # ...\n","])\n","```\n","\n","# Batch normalization #\n","\n","Sljedeći posebni sloj koji ćemo pogledati izvodi \"normalizaciju serije\" (ili \"batchnorm\"), što može pomoći u ispravljanju treninga koji je spor ili nestabilan.\n","\n","S neuronskim mrežama općenito je dobra ideja staviti sve svoje podatke na zajedničku ljestvicu, možda s nečim poput [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) ili [MinMaxScalera](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) scikit-learn-a. Razlog je taj što će SGD pomaknuti mrežne težine proporcionalno veličini aktivacije koju podaci proizvode. Značajke koje proizvode aktivacije vrlo različitih veličina mogu uzrokovati nestabilno ponašanje pri vježbanju.\n","\n","Sada, ako je dobro normalizirati podatke prije nego što odu u mrežu, možda bi i normalizacija unutar mreže bila bolja! Zapravo, imamo posebnu vrstu sloja koji to može učiniti, **sloj normalizacije serije**. Sloj normalizacije serije promatra svaku seriju kako dolazi, prvo normalizirajući seriju s vlastitom srednjom vrijednosti i standardnom devijacijom, a zatim također stavljajući podatke na novu ljestvicu s dva parametra skaliranja koja se mogu obučiti. Batchnorm, zapravo, izvodi neku vrstu koordiniranog skaliranja svojih ulaza.\n","\n","Najčešće se batchnorm dodaje kao pomoć u procesu optimizacije (iako ponekad može pomoći i pri predviđanju izvedbe). Modeli s batchnormom obično trebaju manje epoha za dovršenje obuke. Štoviše, batchnorm također može popraviti razne probleme koji mogu uzrokovati \"zaglavljivanje\" obuke. Razmislite o dodavanju skupne normalizacije svojim modelima, posebno ako imate problema tijekom obuke.\n","\n","## Dodavanje batch normalizacije ##\n","\n","Čini se da se skupna normalizacija može koristiti u gotovo bilo kojoj točki mreže. Možete ga staviti nakon sloja...\n","\n","```\n","layers.Dense(16, activation='relu'),\n","layers.BatchNormalization(),\n","```\n","\n","... ili između sloja i njegove funkcije aktivacije:\n","\n","```\n","layers.Dense(16),\n","layers.BatchNormalization(),\n","layers.Activation('relu'),\n","```\n","\n","A ako ga dodate kao prvi sloj vaše mreže, on može djelovati kao vrsta adaptivnog pretprocesora, zamjenjujući nešto poput `StandardScalera` Sci-Kit Learna.\n","\n","# Primjer - Korištenje dropouta i skupne normalizacije #\n","\n","Nastavimo razvijati model *Red Wine*. Sada ćemo još više povećati kapacitet, ali dodati ispadanje za kontrolu prekomjernog opremanja i normalizaciju serije kako bismo ubrzali optimizaciju. Ovaj put ćemo također izostaviti standardizaciju podataka kako bismo pokazali kako serijska normalizacija može stabilizirati obuku."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"id":"vMiRDcW_sCVP"},"outputs":[],"source":["\n","# Setup plotting\n","import matplotlib.pyplot as plt\n","\n","plt.style.use('seaborn-v0_8-whitegrid')\n","# Set Matplotlib defaults\n","plt.rc('figure', autolayout=True)\n","plt.rc('axes', labelweight='bold', labelsize='large',\n","       titleweight='bold', titlesize=18, titlepad=10)\n","\n","\n","import pandas as pd\n","red_wine = pd.read_csv('./input/red-wine.csv')\n","\n","# Create training and validation splits\n","df_train = red_wine.sample(frac=0.7, random_state=0)\n","df_valid = red_wine.drop(df_train.index)\n","\n","# Split features and target\n","X_train = df_train.drop('quality', axis=1)\n","X_valid = df_valid.drop('quality', axis=1)\n","y_train = df_train['quality']\n","y_valid = df_valid['quality']"]},{"cell_type":"markdown","metadata":{"id":"it5RaQWBsCVP"},"source":["Prilikom dodavanja dropout-a, možda ćete morati povećati broj jedinica u vašim `Dense` slojevima."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9-ohNvbsCVP"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","model = keras.Sequential([\n","    layers.Dense(1024, activation='relu', input_shape=[11]),\n","    layers.Dropout(0.3),\n","    layers.BatchNormalization(),\n","    layers.Dense(1024, activation='relu'),\n","    layers.Dropout(0.3),\n","    layers.BatchNormalization(),\n","    layers.Dense(1024, activation='relu'),\n","    layers.Dropout(0.3),\n","    layers.BatchNormalization(),\n","    layers.Dense(1),\n","])"]},{"cell_type":"markdown","metadata":{"id":"EJz4Q5RisCVP"},"source":["Ovaj put ne trebamo ništa promijeniti u načinu na koji smo postavili trening."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61J4bgEpsCVP"},"outputs":[],"source":["model.compile(\n","    optimizer='adam',\n","    loss='mae',\n",")\n","\n","history = model.fit(\n","    X_train, y_train,\n","    validation_data=(X_valid, y_valid),\n","    batch_size=256,\n","    epochs=100,\n","    verbose=0,\n",")\n","\n","\n","# Show the learning curves\n","history_df = pd.DataFrame(history.history)\n","history_df.loc[:, ['loss', 'val_loss']].plot();"]},{"cell_type":"markdown","metadata":{"id":"1Mqxl1eSsCVQ"},"source":["Obično ćete postići bolje performanse ako standardizirate svoje podatke prije upotrebe za učenje. Međutim, činjenica da smo uopće mogli upotrijebiti neobrađene podatke pokazuje koliko učinkovita može biti skupna normalizacija na težim skupovima podataka.\n","\n","# Zadatak #\n","\n","Prijeđite na [**sljedeći zadatak**](Dropout_and_Batch_Normalization_exercise_hr.ipynb) na *Spotify* skupu podataka s dropoutom i pogledajte kako skupna normalizacija može pomoći s teškim skupovima podataka."]}],"metadata":{"colab":{"name":"Dropout and Batch Normalization","provenance":[]},"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
